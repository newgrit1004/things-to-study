## Table of Contents

- [추천시스템의 이해](#1)
- [컨텐츠 기반 추천](#2)
- [협업 필터링](#3)
- [평가 함수](#4)



## #1
### 추천시스템의 이해
* 추천시스템 개요
    * 추천시스템의 정의는 사용자(user)에게 상품(item)을 제안하는 소프트웨어 도구.
    추천시스템의 목표는 어떤 사용자에게 어떤 상품을 어떠한 방법을 통해 추천할 것인지 결정하여 최종적으로 기업의 매출이나 플랫폼 사용 시간 등의 지표를 높이는 것.



* 기업에서의 추천시스템
    * 당근마켓
        * 다른 사람들이 <b>같이 본 상품</b> 추천.("이 상품과 함께 봤어요" 카테고리)
    * 카카오 (브런치)
        * 해당 글과 <b>유사한 글</b>을 추천.


* 추천시스템 용어
    * implicit feedback
        * 사용자가 상품을 구매했지만, 사용자가 상품 구매에 대한 만족 여부를 알 수 없는 데이터를 의미.(e.g. 대부분의 데이터)
    * explicit feedback
        * 사용자가 상품을 구매한 뒤, 상품 구매에 대한 만족 여부가 포함된 데이터를 의미.(e.g. 영화 평점 데이터)


* 과거의 추천시스템
    * 파레토의 법칙
        * 매출의 80%는 20%의 핵심 고객을 통해 나온다.
    * 롱테일 법칙
        * 인터넷의 발전 덕분에 하위 80%가 상위 20%의 가치보다 커졌다.
    * 연관 분석(Association Analysis)
        * 상품과 상품 사이의 연관을 찾아내는 알고리즘.
        * 연관의 정의
            * 얼마나 같이 구매가 되는가?
            * A 아이템을 구매하는 사람이(조건) B아이템을 구매하는가?
        * 평가 지표
            * 지지도(support) : support(A) = P(A,B)
                * A와 B가 동시에 포함된 거래/전체 거래수.
            * 향상도(lift) : lift(A->B) = P(A,B)/P(A)*P(B)
                * 두 사건이 동시에 얼마나 발생하는지 비율, 독립성을 측정.
            * 신뢰도(confidence) : confidence(A->B) = P(A,B)/P(A)
                * 상품 A를 구매했을 때, 상품 B까지 같이 구매할 확률.
            * 주로 지지도와 신뢰도가 높은 규칙을 선정.

        * 규칙 생성 방식
            * 전체 규칙 탐색 = nCr(r=0~n) = 2^(n)-1
            * A priori 알고리즘
                * 특징
                    * 아이템셋의 증가를 줄이기 위한 알고리즘.
                    * {2, 3}의 지지도 > {0, 2, 3}의 지지도.
                * 알고리즘
                    * 1. k개의 item을 가지고 단일 항목 집단을 생성한다.
                    * 2. 단일 항목 집단에서 사전에 정의한 최소 지지도(support) 이상의 항목만 선택한다.
                    * 3. 2에서 선택된 항목만을 대상으로 2개 항목집단 생성한다.
                    * 4. 2개 항목 집단에서 최소 지지도 또는 신뢰도 이상의 항목만 선택한다.
                    * 5. k개의 k-item frequent set을 생성할 때까지 반복한다.
                * 장점
                    * 원리가 간단하여 알고리즘 사용자가 쉽게 의미 해석이 가능함
                    * 유의한 연관성을 갖는 구매패턴 찾는 것이 가능
                * 단점
                    * 데이터가 많은 경우 연산량이 많아 속도가 느림
                * 라이브러리
                    * mlxtend


            * FP-growth 알고리즘
                * 특징
                    * A priori의 속도 측면의 단점을 FP Tree 구조를 사용하여 개선한 알고리즘.
                * 알고리즘
                    * 1. 모든 거래를 확인하여 각 아이템별 최소 지지도 이상의 아이템만 선택하고, 최소 지지도 미만의 아이템은 거래에서 삭제한다.
                    * 2. 모든 거래에서 거래별로 지지도가 높은 아이템 순서대로 순서를 내림차순으로 정렬한다.
                    * 3. 모든 거래에 대해서 새로운 아이템이 나올 경우 부모 노드로부터 시작하고, 그렇지 않은 경우 기존노드에서 확장하여 아이템 빈도 수를 더해준다.
                    4. 모든 거래를 반영한 FP-Tree가 생성된 뒤, 지지도가 낮은 아이템 순서부터 시작해서, 조건부 패턴을 생성한다.
                    5. 모든 아이템에 대해서 조건부 패턴 생성을 반복한다.
                * 장점
                    * A priori 알고리즘보다 빠르며 2번의 전체 탐색만 필요함.
                * 단점
                    * 대용량 데이터셋에 대해 메모리를 효율적으로 사용하지 않음.
                        * 지지도 계산이 FP-Tree 생성 뒤에 가능함.
                * 라이브러리
                    * mlxtend


## #2
### 컨텐츠 기반 추천
* 컨텐츠 기반 모델
    * 정의
        * 사용자가 이전에 구매한 상품 중에 선호하는 상품들과 유사한 상품을 추천해주는 방법.
    * 방법
        * 아이템을 벡터 형태로 표현하여 표현.
        * 예
            * 텍스트 : BERT, Word2Vec, TF-IDF
            * 이미지 : CNN 등
        * 벡터간의 유사도를 계산.
    * 장점
        * 협업필터링은 다른 사용자들의 평점이 필요하지만, 자신의 평점만으로 추천시스템 만들 수 있음.
        * item의 feature을 통해서 추천하기 때문에, 추천 이유를 설명하기 좋음
        * <b> cold start(e.g. 처음 들어온 사용자, 처음 개봉한 영화)의 케이스에 대해서도 추천이 가능함.</b>
    * 단점
        * 추천시스템의 성능이 item의 feature의 성능에 기반할 수 있음. 따라서 domain knowledge가 필요할 수 있음.
        * 기존 item과 유사한 item만 추천하므로 새로운 장르의 추천이 어려움.
        * 새로운 사용자에 대한 충분한 평점이 쌓이기 전에는 추천하기 힘듬.



* 유사도 함수
    * 유클리디안 유사도
        * 정의
            * 문서 간의 유사도를 계산.
            * 각 문서는 단어가 나타난 횟수로 표현됨.(e.g. doc1= {'과일이':0, '맛있다':2 })
        * formula
            * 1/(유클리디안 거리 + 1e-05)
        * 장점
            * 계산하기 쉬움.
        * 단점
            * p와 q의 분포 또는 범위가 다른 경우.
    * 코사인 유사도
        * 정의
            *
        * formula
            *
        * 장점
            * 벡터의 크기가 중요하지 않은 경우에 사용.
                * 문서 내에서의 단어 빈도수 : 문서들의 길이가 다르더라도, 문서 내에서의 비율을 확인하므로 괜찮음.
        * 단점
            * 벡터의 크기가 중요한 경우 잘 작동하지 않음.
    * 피어슨 유사도
        * 정의
            * #TODO
        * formula
            * #TODO

    * 자카드 유사도
        * 정의
            * 두 집합의 교집합의 크기/두 집합의 합집합의 크기.
    * Divergence 유사도(#TODO)
    * Dice 유사도(#TODO)
    * sorensen 유사도(#TODO)

    * 서로 다른 유사도를 조합하는 방법?
        * 각 유사도별로 추천시스템을 구축한 다음, 결과들을 조합해서 최종 추천을 함.
        * 각 유사도별을 가중치를 줘서 더한 다음에 그걸로 새로운 유사도를 정의한 다음 추천시스템을 구축함.
    * 도메인별 유사도
        * 어떤 아이템에 대한 추천시스템이냐에 따라 유사도가 크게 다르므로 도메인별 유사도 특징을 알아야함.


* TF-IDF 모델
    * 정의
        * 특정 문서 내에서 특정 단어가 얼마나 자주 등장하는지를 의미하는 단어 빈도(TF)와 전체 문서에서 특정 단어가 얼마나 자주 등장하는지를 의미하는 역문서 빈도(DF)를 통해서, 다른 문서에서는 등장하지 않지만 특정 문서에서만 자주 등장하는 단어를 찾아 문서 내 단어 가중치를 계산하는 방법.
    * 용어
        * TF(d,t)
            * 특정 문서 d에서 특정 단어 t의 등장 횟수.
        * DF(t)
            * 특정 단어 t가 등장한 문서의 수.
        * IDF(d,t)
            * idf(d,t) = log(n/(1+df(t)))
        * TF-IDF(d,t)
            * TF-IDF(d,t) = TF(d,t) * IDF(d,t)
    * 사용 이유
        * Counter Vectorizer(TF까지의 과정)라는 방법으로 빈도수가 많이 나오는 단어들에 대해 중요도를 높여줄 수 있다. 하지만 이럴 경우 조사/관사처럼 의미가 없는 단어들의 중요도가 높아지는 문제점이 발생할 수 있다. 따라서 이러한 단어들에 대해서 패널티를 줘서 중요한 단어들을 잡아내는 방법이 TF-IDF 방법이다.
    * 장점
        * 직관적 해석 가능.
    * 단점
        * 대규모 말뭉치를 다룰 때 메모리 문제 발생
            * 높은 차원
            * 매우 sparse한 데이터
    * 라이브러리
        * scikit-learn
            * CountVectorizer
            * TFidfVectorizer


* Word2Vec
    * 정의
        * 원핫인코딩 벡터 형태의 sparse matrix가 가지는 단점을 해소하고자 제안된 방법.
        * "비슷한 위치에 등장하는 단어들은 비슷한 의미를 가진다"라는 가정을 통해 학습.
        * 단어간 유사도를 반영하여 단어를 벡터로 바꿔주는 임베딩 방법론.
        * 추천시스템에서는 단어를 구매 상품으로 바꾸어 구매한 패턴에 word2vec을 적용하여 비슷한 상품을 찾을 수 있음.

    * 기존 통계기반 방법 단점
        * 대규모 말뭉치를 다룰 때, sparse한 데이터를 다루므로 메모리상 문제 발생.
        * 한번에 학습 데이터 전체를 진행함.
            * 큰 작업을 처리하기 어려움.
    * CBOW
        * 주변에 있는 단어를 이용하여 중간에 있는 단어들을 예측하는 방법.
    * Skip-Gram
        * 중간에 있는 단어로 주변 단어를 예측하는 방법.
        * CBOW보다 성능이 좋음.



## #3
### 협업 필터링
* 협업 필터링 모델
    * 정의
        * 사용자의 구매 패턴이나 평점을 가지고 다른 사람들의 구매 패턴, 평점을 통해 추천을 하는 방법
        * 추가적인 사용자의 개인정보나 아이템의 정보 없이도 추천이 가능
        * Netflix Prize Competition에서 우승한 알고리즘
    * 방법
        * 최근접 이웃 기반(Neighborhood based method)
            * KNN
        * 잠재 요인 기반(Latent Factor Collaborative Filtering)
            * SGD
            * ALS
    * 장점
        * 도메인 지식이 필요하지 않음
        * 시작 단계의 모델로 선택하기 좋음
    * 단점
        * 새로운 아이템에 대해 다루기가 힘듬
        * side features(고객의 개인정보, 아이템의 추가정보)를 포함하기 어려움


* Neighborhood based method
    * 머신러닝 알고리즘
        * KNN
            * 정의
                * 새로운 데이터가 들어왔을 때, 새로운 데이터에 대한 클래스를 알아내기 위해 가장 근접한 K개의 neighbors 데이터를 이용하는 방법
    * 알고리즘 종류
        * User-based collaborative filtering
            * 정의
                * 사용자의 구매 패턴 또는 평점과 유사한 사용자를 찾아서 추천 리스트 생성
                * 유사한 사용자를 찾은 뒤, 유사한 사용자가 선호 또는 추천받았던 리스트를 기반으로 추천 리스트 생성
            * 방법
                * 사용자-아이템 explicit feedback이 담겨진 표가 주어졌을 때, 특정 사용자와 다른 사용자간의 유사도를 코사인 유사도 또는 피어슨 유사도를 계산.
                * 만약 표에 explicit feedback이 빈 칸이 있을 경우, 이 값들은 제외하고 유사도를 계산하여, 현재 사용자와 가장 유사도가 높은 N명의 유저들(이하 A) 구함.
                * 사용자의 특정 아이템(이하 I)의 빈 칸을 추정하기 위해서 A가 I에 줬던 explicit feedback 값을 이용.
                * A가 I에 줬던 bias를 제거하기 위해서, 각 사용자별로 explicit feedback average를 제거.

        * Item-based collaborative filtering
            * 정의
                * 특정 사용자가 준 점수 간의 유사한 상품을 찾아서 추천 리스트 생성.
            * 방법
                * User-based collaborative filtering 방법을 item 기반으로 하면 동일.

    * 장점
        * 특정 item을 추천하는 이유를 정당화하기 좋음
        * 새로운 item과 user가 추가되어도 안정적
    * 단점
        * user 기반 방법의 경우 시간, 속도, 메모리가 많이 필요
        * 특정 유저의 top-K에만 관심이 있음
        * 특정 유저의 이웃 중에 아무도 특정 아이템 I를 평가하지 않았을 경우, 특정 유저에게 I에 대한 예측 평가를 제공할 수 없음


* Latent Factor Collaborative Filtering
    * 정의
        * rating matrix에서 사용자와 상품을 잘 표현하는 latent factor를 찾는 방법
        * 사용자-아이템 상호 작용 행렬을 2개의 저차원 직사각형 행렬의 곱으로 분해하여 작동하는 방식
    * 알고리즘 종류
        * SGD
            * 정의
                * 고유값 분해(eigen value decomposition)와 같은 행렬을 대각화하는 방법
                * objective function은 <b>실제 평점 matrix와 유저 latent 행렬과 아이템 latent 행렬의 곱의 차이를 최소화하는 것</b>
            * 방법
                * 1. User Latent와 Item Latent를 임의로 초기화한다. 이때, User Latent(U)의 row 개수는 user의 수, Item Latent(V)의 Transponse행렬의 column 개수는 Item의 수이다. U의 column 개수와 V의 row 개수는 같으며 유저가 임의로 정한다. 일반적으로 숫자 20을 사용한다.
                * 2. 실제 rating matrix의 값과 추정된 rating matrix의 값의 차이를 계산하며 gradient descent를 진행한다.
                * 3. 모든 평점에 대해 2번의 과정을 반복하며(epoch 1), 만약 실제 rating matrix에 빈 칸이 존재하면, 이것은 계산을 스킵한다.
            * 결과 사용 방법
                * 학습이 끝난 이후, predicted rating matrix를 이용해서 기존에 빈 칸이었던 값들을 채워서 평가 이후 추천여부 결정 가능
                * 학습이 잘 된 User Latent나 Item Latent를 기반으로 추천해주는 것도 가능
            * 장점
                * 다른 loss function 사용하는 것도 가능
                * 병렬처리 가능
            * 단점
                * 수렴 속도가 느림
        * ALS
            * 정의
                * 기존의 SGD가 두개의 행렬(User Latent, Item Latent)을 동시에 최적화하는 방법이라면, ALS는 두 행렬 중 하나를 고정시키고 다른 하나의 행렬을 순차적으로 반복하면서 최적화하는 방법
                * 두 행렬 중 하나의 행렬을 고정시키고 다른 행렬을 최적화하므로 수렴된 행렬을 찾을 수 있다는 장점이 있음
            * 방법
                * 1. User Latent, Item Latent를 초기화
                * 2. Item Latent를 고정하고 User Latent를 최적화
                * 3. User Latent를 고정하고 Item Latent를 최적화
                * 4. 2,3번 과정 반복
                * 5. 빈 칸은 모두 0으로 바꿔줌
            * 장점
                * SGD보다 수렴속도 빠름
                * parallelized가 가능함
            * 단점
                * loss square만 사용 가능


## #4
### 평가 함수
* 평가 함수를 다양하게 알아야 하는 이유
    * 시나리오
        * 내가 추천해준 영화를 고객이 보았는가?
            * 실제 고객의 만족도가 낮을 수도 있음
            * 유튜브 같은 경우는 체류 시간을 메트릭으로 사용
        * 내가 추천해준 영화를 고객이 높은 점수로 평점을 주었는가?
    * 지표
        * 정확도(Accuracy)
            * 내가 추천해준 아이템 중 고객이 본 아이템의 개수로 평가
                * 추천하지 않은 아이템의 개수는 추천한 아이템의 개수에 비해 굉장히 많기 때문
                * 상위 n개의 상품을 추천하였을 때 정확도를 기준으로 판단
        * MAP
            * Precision
                * True Positive/(True Positive + False Positive)
            * AP(Average Precision)
                * 추천한 K개의 item의 precision을 평균낸 것
            * MAP(Mean Average Precision)
                * N명의 사용자의 AP를 평균낸 값
                * 추천의 순서에 따라 값이 차이가 남
                * 상위 k개의 추천에 대해서만 평가
            * NDCG(Normalized Discounted Cumulative Gain)
                * 검색 알고리즘 랭킹 퀄리티를 측정하는 메트릭
                * 추천엔진은 user와 연관있는 item을 추천해주므로, 검색 작업을 수행하는 것이라 생각하는 것이 가능
                * CG
                    * A = [3,2,1], B=[1,2,3] 일때, 둘의 CG는 list 안의 모든 원소를 더한 값인 6이라 생각 가능
                    * A가 B보다 나은 추천 결과이지만, CG에서는 순서를 고려하지 못함
                * DCG
                    * 분모에 log2(idx+1)을 설정해서 더해주므로써 처음에 맞췄을 때 가중치를 더 많이 받도록 설정가능
                * NDCG
                    * 이상적인 순서 = Ideal Order = 주어진 추천 순서에서 relevance 기준으로 내림차순한 것(가장 좋을 수 있는 추천 순서)
                    * NDCG = DCG/Ideal DCG




# References
- [[토크ON세미나] 추천시스템 분석 입문하기](https://www.youtube.com/watch?v=43gb7WK56Sk&list=PL9mhQYIlKEhdkOVTZWJJIy8rv6rQaZNNc&index=1)




